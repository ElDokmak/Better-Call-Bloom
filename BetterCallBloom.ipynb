{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"pile-of-law/pile-of-law\",'r_legaladvice')"
      ],
      "metadata": {
        "id": "hpL9ek8a7pfM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-3b\")\n",
        "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-3b\")"
      ],
      "metadata": {
        "id": "Uu-aK4WneQ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        " k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_datasets = tokenized_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=8,\n",
        ")"
      ],
      "metadata": {
        "id": "N5AOSH8TehlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=lm_datasets[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Vz4bNIxpfsOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BloomTokenizerFast.from_pretrained(\"tomrb/bettercallbloom-3b\")\n",
        "model = BloomForCausalLM.from_pretrained(\"tomrb/bettercallbloom-3b\",low_cpu_mem_usage=True)\n",
        "\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer,do_sample=False)\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    #We add 'Question :' and 'Answer #1:' at the start and end of the prompt\n",
        "    return \"\\nQuestion: \" + text + \"\\nAnswer #1:\"\n",
        "\n",
        "\n",
        "def generate(text):\n",
        "    \n",
        "    preprocessed_text = preprocess(text)\n",
        "    result = generator(preprocessed_text, max_length=128)\n",
        "    output = re.split(r'\\nQuestion:|Answer #1:|Answer #|Title:',result[0]['generated_text'])[2]\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "IKLuPoYNfvQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "  input_text = gr.Textbox(label=\"Input\", lines=6)  \n",
        "  buton = gr.Button(\"Submit \")  \n",
        "  output_text = gr.Textbox(lines=6, label=\"Output\")\n",
        "  buton.click(generate, inputs=[input_text], outputs=output_text)  \n",
        "\n",
        "demo.launch(enable_queue=True, debug=True)"
      ],
      "metadata": {
        "id": "tdi8uW2WfxdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}